{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2372b3f6-7a1e-4afb-84a0-fc9debc10c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43068ee3-a1dd-4c66-9efe-767bb9f4f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f0911-c6f3-4ff4-a684-753e80a2d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import get_scheduler\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForSequenceClassification \n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "import gc\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import tokenizers\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import scipy as sp\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, log_loss, precision_score, recall_score, confusion_matrix, matthews_corrcoef, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import wandb\n",
    "# CPMP: declare the two GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "# CPMP: avoids some issues when using more than one worker\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import matplotlib.pyplot as plt\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "from transformers import EsmModel\n",
    "from transformers import EsmTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c95b26-cc35-4cf6-9e75-039575bc11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def protbert_prediction(file_path):\n",
    "    # define the path and output directory\n",
    "    Output_Directory = f'PROTBERT/'\n",
    "    if not os.path.exists(Output_Directory):\n",
    "        os.makedirs(Output_Directory)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # configuration for the protbert model\n",
    "    class CFG:\n",
    "        apex=True\n",
    "        print_freq=20\n",
    "        num_workers=1 \n",
    "        model=\"Rostlab/prot_bert\"\n",
    "        gradient_checkpointing=False\n",
    "        scheduler='cosine' # ['linear', 'cosine', 'constant']\n",
    "        batch_scheduler=True\n",
    "        num_warmup_steps=0\n",
    "        epochs=6\n",
    "        num_cycles=1\n",
    "        encoder_lr=4.09e-5\n",
    "        decoder_lr= 4.49e-5\n",
    "        batch_size=32\n",
    "        dropout = 0\n",
    "        # Model information for the rostlab/prot_bert model\n",
    "        total_layers = 30 # found from the explanation below\n",
    "        initial_layers = 5 \n",
    "        layers_per_block = 16 \n",
    "        \n",
    "        # Parameters of the model can be frozen, total parameters of a model can be found after loading it using:\n",
    "        #for i,(name, param) in enumerate(list(model.named_parameters())): print(i,name)\n",
    "        # parameters can then be frozen using:\n",
    "        #for name, param in list(model.named_parameters())\\[:CFG.initial_layers+CFG.layers_per_block*CFG.num_freeze_layers]: param.requires_grad = False\n",
    "        # Setting number of frozen layers:\n",
    "        num_freeze_layers = total_layers - 2\n",
    "        #configuration of the model parameters, the seed, folds for groupkfold splitting, \n",
    "        #as well as the dimension reduction used by the second last linear layer for embedding dimension reduction.\n",
    "        min_lr=1e-6\n",
    "        eps=1e-08\n",
    "        betas=(0.9, 0.999)\n",
    "        max_len=100\n",
    "        weight_decay=0.00175\n",
    "        gradient_accumulation_steps=4\n",
    "        max_grad_norm=1000\n",
    "        target_cols=['labels']\n",
    "        seed=42\n",
    "        n_fold=10\n",
    "        trn_fold=[0,1,2,3,4,5,6,7,8,9]\n",
    "        train=True\n",
    "        pca_dim = 256\n",
    "    # Tokenizer for protbert\n",
    "    tokenizer = AutoTokenizer.from_pretrained('PROTBERT/tokenizer/')\n",
    "    CFG.tokenizer = AutoTokenizer.from_pretrained('PROTBERT/tokenizer/')\n",
    "    \n",
    "    # model\n",
    "    class MeanPooling(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MeanPooling, self).__init__()    \n",
    "        def forward(self, last_hidden_state, attention_mask):\n",
    "            #adds a new dimension after the last index of the attention_mask, then expands it to the size of the last hidden state.   \n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            #clamps all elements in the sum_mask such that the minimum value is 1e-9, making any zeroes a small number instead.\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            #returns the mean of the embeddings\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            return mean_embeddings\n",
    "    \n",
    "\n",
    "    class CustomModel(nn.Module):\n",
    "        def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "            super().__init__()\n",
    "            self.cfg = cfg\n",
    "            #configuration for the model, loads the model specified in the config section at the top of the notebook.\n",
    "            if config_path is None:\n",
    "                self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "                self.config.hidden_dropout = cfg.dropout\n",
    "                self.config.hidden_dropout_prob = cfg.dropout\n",
    "                self.config.attention_dropout = cfg.dropout\n",
    "                self.config.attention_probs_dropout_prob = cfg.dropout\n",
    "            else:\n",
    "                self.config = torch.load(config_path)\n",
    "            if pretrained:\n",
    "                self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "            else:\n",
    "                self.model = AutoModel.from_config(self.config)\n",
    "            \n",
    "            if self.cfg.gradient_checkpointing:\n",
    "                self.model.gradient_checkpointing_enable()\n",
    "            \n",
    "            #returns the mean of the embeddings, as specified in the MeanPooling() function.\n",
    "            self.pool = MeanPooling()\n",
    "            #linear layer, reduces the dimenzionality of the hidden_size (in this case 1024) specified in the config file, \n",
    "            #and reduces it to the pca_dim specified in the config file (In this case 64)\n",
    "            self.fc1 = nn.Linear(self.config.hidden_size, self.cfg.pca_dim)\n",
    "            #second linear layer, reduces the dimensonality from the pca_dim*6 (6, for 6 features) to 1 (dTm) nn.Linear changed to nn.Softmax for classification\n",
    "            self.fc2 = nn.Linear(self.cfg.pca_dim, 1)\n",
    "            #initializes weights of the linear layers\n",
    "            self._init_weights(self.fc1)\n",
    "            self._init_weights(self.fc2)\n",
    "        \n",
    "        \n",
    "        def _init_weights(self, module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "                if module.padding_idx is not None:\n",
    "                    module.weight.data[module.padding_idx].zero_()\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "        \n",
    "        def feature(self, batch):\n",
    "            # CPMP comment: pass inputs explicitly\n",
    "            outputs = self.model(batch['input_ids'],batch['attention_mask'])\n",
    "            last_hidden_states = outputs[0]\n",
    "            feature = self.pool(last_hidden_states, batch['attention_mask'])\n",
    "            return feature\n",
    "\n",
    "        def forward(self, batch):\n",
    "            # CPMP comment: change code to read inputs from a single dictionary\n",
    "            feature = self.fc1(self.feature(batch))\n",
    "        \n",
    "            #feature = torch.cat((feature),axis=-1)\n",
    "            output = self.fc2(feature)\n",
    "            return output\n",
    "        # prepare the sequence input for prediction by the pretrained protbert model\n",
    "    def prepare_input(cfg, text):\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text, \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=cfg.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "    # test dataset\n",
    "    CFG.path = Output_Directory\n",
    "    CFG.config_path = CFG.path+'config.pth' \n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, cfg, df):\n",
    "            self.cfg = cfg\n",
    "            self.text = df['sequence'].values\n",
    "        def __len__(self):\n",
    "            return len(self.text)\n",
    "\n",
    "        def __getitem__(self, item):\n",
    "            #prepares the text inputs for use in the bert model (tokenize, pad, truncate, encode)\n",
    "            inputs1 = prepare_input(self.cfg, self.text[item])\n",
    "            #gets the labels for each item\n",
    "            #returns a dict of all inputs\n",
    "            return {'input_ids' : inputs1['input_ids'], \n",
    "                    'attention_mask' : inputs1['attention_mask']}\n",
    "    # inference on file\n",
    "    def inference_fn(test_loader, model, device):\n",
    "        preds = []\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        # CPMP: using 2 GPU\n",
    "        #model = nn.DataParallel(model)\n",
    "        tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "        # CPMP: iterating on batch dictionaries\n",
    "        for batch in tk0:\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(batch)\n",
    "            preds.append(y_preds.to('cpu').numpy())\n",
    "        predictions = np.concatenate(preds)\n",
    "        return predictions\n",
    "    # add spaces to the FASTA sequence, protein NLP models were trained on space separated sequences (A G A G)\n",
    "    def add_spaces(x):\n",
    "        return \" \".join(list(x))\n",
    "\n",
    "    # load the test file\n",
    "    def load_test(file_path):\n",
    "        with open(file_path) as fasta_file:\n",
    "            identifiers = []\n",
    "            lengths = []\n",
    "            seq = []\n",
    "            for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
    "                seq.append(str(seq_record.seq))\n",
    "                identifiers.append(seq_record.id)\n",
    "                lengths.append(len(seq_record.seq))\n",
    "        test = pd.DataFrame()\n",
    "        test['sequence'] = seq\n",
    "        test['len'] = lengths\n",
    "        test['identifiers'] = identifiers\n",
    "        test.drop_duplicates(inplace=True)\n",
    "        test.reset_index(drop=True,inplace=True)\n",
    "        test['sequence'] = test.sequence.map(add_spaces)\n",
    "        return test\n",
    "    # protbert prediction on the fasta file\n",
    "    def pred(test):\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_loader = DataLoader(test_dataset,\n",
    "                                 batch_size=CFG.batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "        predictions_ = []\n",
    "        for fold in CFG.trn_fold:\n",
    "            model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n",
    "            state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                               map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(state['model'])\n",
    "            prediction = inference_fn(test_loader, model, device)\n",
    "            predictions_.append(prediction)\n",
    "            del model, state, prediction; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        predictions = np.mean(predictions_, axis=0)\n",
    "        test['predictions']  = predictions\n",
    "        return predictions\n",
    "    #load the fasta file\n",
    "    test = load_test(file_path)\n",
    "    #protbert predict the fasta sequences\n",
    "    test['predictions'] = pred(test)\n",
    "    test['protbert'] =torch.sigmoid(torch.tensor(test['predictions']))\n",
    "    #save a csv for the protbert predictions\n",
    "    test.to_csv('protbert_prediction')\n",
    "    return test['protbert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bb880-c0fd-4e7a-83b1-4d443e3a8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#protbert_predictions = protbert('XUAMP_testing_datasets/XU_AMP.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34035e1-ea6f-494a-a130-4b5eb930a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_prediction(file_path):\n",
    "    # define the path and output directory\n",
    "    Output_Directory = f'ESM/'\n",
    "    if not os.path.exists(Output_Directory):\n",
    "        os.makedirs(Output_Directory)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # configuration for the esm model\n",
    "    class CFG:\n",
    "        apex=True\n",
    "        print_freq=20\n",
    "        num_workers=1 \n",
    "        model=\"facebook/esm2_t33_650M_UR50D\"\n",
    "        gradient_checkpointing=False\n",
    "        scheduler='cosine' # ['linear', 'cosine', 'constant']\n",
    "        batch_scheduler=True\n",
    "        num_warmup_steps=0\n",
    "        epochs=6\n",
    "        num_cycles=1\n",
    "        encoder_lr=4.09e-5\n",
    "        decoder_lr= 4.49e-5\n",
    "        batch_size=32\n",
    "        dropout = 0\n",
    "        total_layers = int(model.split('_')[1][1:])\n",
    "        initial_layers = 2 \n",
    "        layers_per_block = 16\n",
    "        num_freeze_layers = total_layers - 2\n",
    "        min_lr=1e-6\n",
    "        eps=1e-08\n",
    "        betas=(0.9, 0.999)\n",
    "        max_len=100\n",
    "        weight_decay=0.00175\n",
    "        gradient_accumulation_steps=4\n",
    "        max_grad_norm=1000\n",
    "        target_cols=['labels']\n",
    "        seed=42\n",
    "        n_fold=10\n",
    "        trn_fold=[0,1,2,3,4,5,6,7,8,9]\n",
    "        train=True\n",
    "        pca_dim = 256\n",
    "    # Tokenizer for esm\n",
    "    tokenizer = AutoTokenizer.from_pretrained('ESM/tokenizer/')\n",
    "    CFG.tokenizer = AutoTokenizer.from_pretrained('ESM/tokenizer/')\n",
    "    \n",
    "    # model\n",
    "    class MeanPooling(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MeanPooling, self).__init__()    \n",
    "        def forward(self, last_hidden_state, attention_mask):\n",
    "            #adds a new dimension after the last index of the attention_mask, then expands it to the size of the last hidden state.   \n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            #clamps all elements in the sum_mask such that the minimum value is 1e-9, making any zeroes a small number instead.\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            #returns the mean of the embeddings\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            return mean_embeddings\n",
    "    \n",
    "\n",
    "    class CustomModel(nn.Module):\n",
    "        def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "            super().__init__()\n",
    "            self.cfg = cfg\n",
    "            #configuration for the model, loads the model specified in the config section at the top of the notebook.\n",
    "            if config_path is None:\n",
    "                self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "                self.config.hidden_dropout = cfg.dropout\n",
    "                self.config.hidden_dropout_prob = cfg.dropout\n",
    "                self.config.attention_dropout = cfg.dropout\n",
    "                self.config.attention_probs_dropout_prob = cfg.dropout\n",
    "            else:\n",
    "                self.config = torch.load(config_path)\n",
    "            if pretrained:\n",
    "                self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "            else:\n",
    "                self.model = AutoModel.from_config(self.config)\n",
    "            \n",
    "            if self.cfg.gradient_checkpointing:\n",
    "                self.model.gradient_checkpointing_enable()\n",
    "            \n",
    "            #returns the mean of the embeddings, as specified in the MeanPooling() function.\n",
    "            self.pool = MeanPooling()\n",
    "            #linear layer, reduces the dimenzionality of the hidden_size (in this case 1024) specified in the config file, \n",
    "            #and reduces it to the pca_dim specified in the config file (In this case 64)\n",
    "            self.fc1 = nn.Linear(self.config.hidden_size, self.cfg.pca_dim)\n",
    "            #second linear layer, reduces the dimensonality from the pca_dim*6 (6, for 6 features) to 1 (dTm) nn.Linear changed to nn.Softmax for classification\n",
    "            self.fc2 = nn.Linear(self.cfg.pca_dim, 1)\n",
    "            #initializes weights of the linear layers\n",
    "            self._init_weights(self.fc1)\n",
    "            self._init_weights(self.fc2)\n",
    "        \n",
    "        \n",
    "        def _init_weights(self, module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "                if module.padding_idx is not None:\n",
    "                    module.weight.data[module.padding_idx].zero_()\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "        \n",
    "        def feature(self, batch):\n",
    "            # CPMP comment: pass inputs explicitly\n",
    "            outputs = self.model(batch['input_ids'],batch['attention_mask'])\n",
    "            last_hidden_states = outputs[0]\n",
    "            feature = self.pool(last_hidden_states, batch['attention_mask'])\n",
    "            return feature\n",
    "\n",
    "        def forward(self, batch):\n",
    "            # CPMP comment: change code to read inputs from a single dictionary\n",
    "            feature = self.fc1(self.feature(batch))\n",
    "        \n",
    "            #feature = torch.cat((feature),axis=-1)\n",
    "            output = self.fc2(feature)\n",
    "            return output\n",
    "        # prepare the sequence input for prediction by the pretrained protbert model\n",
    "    def prepare_input(cfg, text):\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text, \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=cfg.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "    # test dataset\n",
    "    CFG.path = Output_Directory\n",
    "    CFG.config_path = CFG.path+'config.pth' \n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, cfg, df):\n",
    "            self.cfg = cfg\n",
    "            self.text = df['sequence'].values\n",
    "        def __len__(self):\n",
    "            return len(self.text)\n",
    "\n",
    "        def __getitem__(self, item):\n",
    "            #prepares the text inputs for use in the bert model (tokenize, pad, truncate, encode)\n",
    "            inputs1 = prepare_input(self.cfg, self.text[item])\n",
    "            #gets the labels for each item\n",
    "            #returns a dict of all inputs\n",
    "            return {'input_ids' : inputs1['input_ids'], \n",
    "                    'attention_mask' : inputs1['attention_mask']}\n",
    "    # inference on file\n",
    "    def inference_fn(test_loader, model, device):\n",
    "        preds = []\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        # CPMP: using 2 GPU\n",
    "        #model = nn.DataParallel(model)\n",
    "        tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "        # CPMP: iterating on batch dictionaries\n",
    "        for batch in tk0:\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(batch)\n",
    "            preds.append(y_preds.to('cpu').numpy())\n",
    "        predictions = np.concatenate(preds)\n",
    "        return predictions\n",
    "    # add spaces to the FASTA sequence, protein NLP models were trained on space separated sequences (A G A G)\n",
    "    def add_spaces(x):\n",
    "        return \" \".join(list(x))\n",
    "\n",
    "    # load the test file\n",
    "    def load_test(file_path):\n",
    "        with open(file_path) as fasta_file:\n",
    "            identifiers = []\n",
    "            lengths = []\n",
    "            seq = []\n",
    "            for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
    "                seq.append(str(seq_record.seq))\n",
    "                identifiers.append(seq_record.id)\n",
    "                lengths.append(len(seq_record.seq))\n",
    "        test = pd.DataFrame()\n",
    "        test['sequence'] = seq\n",
    "        test['len'] = lengths\n",
    "        test['identifiers'] = identifiers\n",
    "        test.drop_duplicates(inplace=True)\n",
    "        test.reset_index(drop=True,inplace=True)\n",
    "        test['sequence'] = test.sequence.map(add_spaces)\n",
    "        return test\n",
    "    # protbert prediction on the fasta file\n",
    "    def pred(test):\n",
    "        test_dataset = TestDataset(CFG, test)\n",
    "        test_loader = DataLoader(test_dataset,\n",
    "                                 batch_size=CFG.batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=CFG.num_workers, pin_memory=True, drop_last=False)  \n",
    "        predictions_ = []\n",
    "        for fold in CFG.trn_fold:\n",
    "            model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n",
    "            state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n",
    "                               map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(state['model'])\n",
    "            prediction = inference_fn(test_loader, model, device)\n",
    "            predictions_.append(prediction)\n",
    "            del model, state, prediction; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        predictions = np.mean(predictions_, axis=0)\n",
    "        test['predictions']  = predictions\n",
    "        return predictions\n",
    "    #load the fasta file\n",
    "    test = load_test(file_path)\n",
    "    #protbert predict the fasta sequences\n",
    "    test['predictions'] = pred(test)\n",
    "    test['esm'] =torch.sigmoid(torch.tensor(test['predictions']))\n",
    "    #save a csv for the protbert predictions\n",
    "    test.to_csv('esm_prediction')\n",
    "    return test['esm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343a181-f314-4e67-abed-6aeb7df8377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amp_predictor("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
